---
title: "Figure 1 and related supplement tables and figures"
author: "Meng-Ching Ko (MaggieMCKO)"
date: "07/17/2024"
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

This [R Markdown](http://rmarkdown.rstudio.com) Notebook contain codes for reproducing Fig.1 and related supplement tables and figures of Ko et al. (https://www.biorxiv.org/content/10.1101/2022.06.13.495861v3).
Data deposited on dryad: https://doi.org/10.5061/dryad.5hqbzkh8c

### Fig. 1B, Supplementary Table 2 and 3
```{r}
library(tidyverse) # v.2.0.0
library(lme4) # 1.1-29
library(arm) # 1.7-19
set.seed(100)

## load data
path =  paste0(getwd(), "/Data/PlasmaAndrogenLv.tsv")
T_sub = read_tsv(path)

db = T_sub

db$Group <- factor(db$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))
db$Group <- droplevels(db$Group)
db$ProcessNum <- factor(db$ProcessNum)
db$ProcessNum <- droplevels(db$ProcessNum)
db$date <- factor(db$date)
db$date <- droplevels(db$date)
db$PrePost <- factor(db$PrePost)
db$PrePost <- droplevels(db$PrePost)


y_lab = 'Plasma androgens (ng/ml)'

## deciding using sqrt or not and evaluating model
mod_a <- lmer(data = db, log(ngml) ~ PrePost + Group + PrePost:Group +
                (1|date) + (1|ProcessNum)) # log
plot(mod_a)  
```

```{r}
summary(mod_a)

plot.new()
par(mfrow=c(3,2))
qqnorm(resid(mod_a))
qqline(resid(mod_a))
qqnorm(ranef(mod_a)$"ProcessNum"[,1])   # random effect
qqline(ranef(mod_a)$"ProcessNum"[,1])
qqnorm(ranef(mod_a)$"date"[,1])   # random effect
qqline(ranef(mod_a)$"date"[,1])
scatter.smooth(fitted(mod_a), resid(mod_a))
```

```{r}
## Draw 200 random values from the posterior distribuition of the model
nsim = 10000 # 200 # 
bsim = sim(mod_a, n.sims = nsim) # curve
# str(bsim)

## Check the credible intervals
Ind_b_CrI = apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975)) 

Ind_b = mod_a@beta
x = as.data.frame(VarCorr(mod_a))
Ind_random_Bird = x[1, 4]
Ind_random_Date = x[2, 4]
Ind_residual = x[3, 4]
Ind_random_Bird_CrI = quantile(apply(bsim@ranef$ProcessNum[,,1],1,var),prob=c(0.025, 0.975))
Ind_random_Date_CrI = quantile(apply(bsim@ranef$date[,,1],1,var),prob=c(0.025, 0.975))
Ind_residual_CrI = quantile(bsim@sigma^2,c(0.025, 0.975))

x = cbind("Var" = "T_ngml", "Estimate" = Ind_b, t(Ind_b_CrI))
df_t = rbind(cbind(x, "Parameter" = row.names(x)),
             cbind("Var" = "T_ngml", 
                   "Estimate" =  Ind_random_Date, t(Ind_random_Date_CrI),
                   "Parameter" = "random_Date"),
             cbind("Var" = "T_ngml", 
                   "Estimate" =  Ind_random_Bird, t(Ind_random_Bird_CrI),
                   "Parameter" = "random_Bird"),
             cbind("Var" = "T_ngml", 
                   "Estimate" =  Ind_residual, t(Ind_residual_CrI),
                   "Parameter" = "residual"))

## Draw dummy values from the raw data
groups = sort(unique(db$Group))
new_data_mod_a = data.frame(
  PrePost = rep(unique(T_sub$PrePost), each=length(groups)),
  Group = rep(groups, 2))

## Simulate dummy values that can be multiplied later
xmat = model.matrix(~ PrePost + Group + PrePost:Group, data = new_data_mod_a)

## Empty matrix to store results of the multiplication
fitmat = matrix(ncol = nsim, nrow=nrow(new_data_mod_a))

## Multiply dummy values and the model and store in the empty matrix
for(i in 1:nsim){
  fitmat[,i] = xmat%*%bsim@fixef[i,]}

## Calculate the upper and lower limits of the credible interval and back transform
new_data_mod_a$lower <- apply(fitmat, 1, quantile, prob=0.025)
new_data_mod_a$upper <- apply(fitmat, 1, quantile, prob=0.975)
new_data_mod_a$fit <- apply(fitmat, 1, mean)
new_data_mod_a$fit_exp <- exp(new_data_mod_a$fit)        # transform back from log
new_data_mod_a$upper_exp <- exp(new_data_mod_a$upper)    # transform back from log
new_data_mod_a$lower_exp <- exp(new_data_mod_a$lower)    # transform back from log

# grey_point_size = 0.2
mean_point_size = 1
errorbar_wd = 1
errorbar_hori = 0

w = 0.25
Tplot = ggplot(data=db) +
  # raw data
  geom_point(aes(x=as.numeric(Group)+w, y=ngml, # shape = PrePost, 
                 colour=PrePost, group=PrePost), 
             size = 0.5, alpha = 0.7, 
             position=position_jitter(w = 0.05, h = 0)) +
  # Credible interval
  geom_errorbar(data = new_data_mod_a,
                aes(x = as.numeric(Group), ymax=upper_exp, ymin=lower_exp,
                    colour=PrePost, group=PrePost),
                width= errorbar_hori, size = errorbar_wd ) +
  # estimate
  geom_point(data = new_data_mod_a,
             aes(x = as.numeric(Group), y = fit_exp,
                 color = PrePost, group=PrePost, fill = PrePost),
             stroke = 0.2, color = 'black', size = mean_point_size) +
  scale_x_continuous("Group", breaks=c(1:7),
                     label=c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))+
  scale_color_manual(values = c("cornflowerblue", "darkorange") ) +
  scale_y_continuous(name = 'Plasma androgens (ng/ml)',
                     expand = c(0, 0),
                     limits = c(-5, 120)) + theme_classic() ; Tplot
```


```{r}
pp = function(StimulatedTable){
  Individualb1 <- new_data_mod_a$Group=='CON' & new_data_mod_a$PrePost=='before'
  Individualb2 <- new_data_mod_a$Group=='T1h' & new_data_mod_a$PrePost=='before'
  Individualb3 <- new_data_mod_a$Group=='T3h' & new_data_mod_a$PrePost=='before'
  Individualb4 <- new_data_mod_a$Group=='T8h' & new_data_mod_a$PrePost=='before'
  Individualb5 <- new_data_mod_a$Group=='T3d' & new_data_mod_a$PrePost=='before'
  Individualb6 <- new_data_mod_a$Group=='T7d' & new_data_mod_a$PrePost=='before'
  Individualb7 <- new_data_mod_a$Group=='T14d' & new_data_mod_a$PrePost=='before'
  
  Individuala1 <- new_data_mod_a$Group=='CON' & new_data_mod_a$PrePost=='sacrifice'
  Individuala2 <- new_data_mod_a$Group=='T1h' & new_data_mod_a$PrePost=='sacrifice'
  Individuala3 <- new_data_mod_a$Group=='T3h' & new_data_mod_a$PrePost=='sacrifice'
  Individuala4 <- new_data_mod_a$Group=='T8h' & new_data_mod_a$PrePost=='sacrifice'
  Individuala5 <- new_data_mod_a$Group=='T3d' & new_data_mod_a$PrePost=='sacrifice'
  Individuala6 <- new_data_mod_a$Group=='T7d' & new_data_mod_a$PrePost=='sacrifice'
  Individuala7 <- new_data_mod_a$Group=='T14d' & new_data_mod_a$PrePost=='sacrifice'
  
  # sacrifice > before
  d1 = mean(fitmat[Individuala1,]>fitmat[Individualb1,]) # posterior probability 
  d2 = mean(fitmat[Individuala2,]>fitmat[Individualb2,]) # posterior probability 
  d3 = mean(fitmat[Individuala3,]>fitmat[Individualb3,]) # posterior probability 
  d4 = mean(fitmat[Individuala4,]>fitmat[Individualb4,]) # posterior probability 
  d5 = mean(fitmat[Individuala5,]>fitmat[Individualb5,]) # posterior probability 
  d6 = mean(fitmat[Individuala6,]>fitmat[Individualb6,]) # posterior probability 
  d7 = mean(fitmat[Individuala7,]>fitmat[Individualb7,]) # posterior probability 
  
  # sacrifice > sacrifice (CON)
  ds2 = mean(fitmat[Individuala2,]>fitmat[Individuala1,]) # posterior probability 
  ds3 = mean(fitmat[Individuala3,]>fitmat[Individuala1,]) # posterior probability 
  ds4 = mean(fitmat[Individuala4,]>fitmat[Individuala1,]) # posterior probability 
  ds5 = mean(fitmat[Individuala5,]>fitmat[Individuala1,]) # posterior probability 
  ds6 = mean(fitmat[Individuala6,]>fitmat[Individuala1,]) # posterior probability 
  ds7 = mean(fitmat[Individuala7,]>fitmat[Individuala1,]) # posterior probability 
  
  # sacrifice > sacrifice (T1h)
  ds32 = mean(fitmat[Individuala3,]>fitmat[Individuala2,]) # posterior probability 
  ds42 = mean(fitmat[Individuala4,]>fitmat[Individuala2,]) # posterior probability 
  ds52 = mean(fitmat[Individuala5,]>fitmat[Individuala2,]) # posterior probability 
  ds62 = mean(fitmat[Individuala6,]>fitmat[Individuala2,]) # posterior probability 
  ds72 = mean(fitmat[Individuala7,]>fitmat[Individuala2,]) # posterior probability 
  
  # sacrifice > sacrifice (T3h)
  ds43 = mean(fitmat[Individuala4,]>fitmat[Individuala3,]) # posterior probability 
  ds53 = mean(fitmat[Individuala5,]>fitmat[Individuala3,]) # posterior probability 
  ds63 = mean(fitmat[Individuala6,]>fitmat[Individuala3,]) # posterior probability 
  ds73 = mean(fitmat[Individuala7,]>fitmat[Individuala3,]) # posterior probability 
  
  # sacrifice > sacrifice (T8h)
  ds54 = mean(fitmat[Individuala5,]>fitmat[Individuala4,]) # posterior probability 
  ds64 = mean(fitmat[Individuala6,]>fitmat[Individuala4,]) # posterior probability 
  ds74 = mean(fitmat[Individuala7,]>fitmat[Individuala4,]) # posterior probability 
  
  # sacrifice > sacrifice (T3d)
  ds65 = mean(fitmat[Individuala6,]>fitmat[Individuala5,]) # posterior probability 
  ds75 = mean(fitmat[Individuala7,]>fitmat[Individuala5,]) # posterior probability 
  
  # sacrifice > sacrifice (T7d)
  ds76 = mean(fitmat[Individuala7,]>fitmat[Individuala6,]) # posterior probability 
  
  group = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d")
  dt = data.frame("Comparison" = c(paste0(group, '(sacrifice > before)'),
                                   paste0(group[-1], '(sacrifice > sacrifice (', group[1], '))'),
                                   paste0(group[-c(1:2)], '(sacrifice > sacrifice (', group[2], '))'),
                                   paste0(group[-c(1:3)], '(sacrifice > sacrifice (', group[3], '))'),
                                   paste0(group[-c(1:4)], '(sacrifice > sacrifice (', group[4], '))'),
                                   paste0(group[-c(1:5)], '(sacrifice > sacrifice (', group[5], '))'),
                                   paste0(group[-c(1:6)], '(sacrifice > sacrifice (', group[6], '))')
  ), 
  "Posterior probability" = c(d1, d2, d3, d4, d5, d6, d7,
                              ds2, ds3, ds4, ds5, ds6, ds7,
                              ds32, ds42, ds52, ds62, ds72,
                              ds43, ds53, ds63, ds73,
                              ds54, ds64, ds74,
                              ds65, ds75,
                              ds76
  ))
}

# posterior probability 
plasmaT = pp(new_data_mod_a) # posterior probabilities
plasmaT = plasmaT %>% mutate(sig = ifelse(Posterior.probability > 0.95, 'sig', NA)); plasmaT
```
```{r}
# estimates and credible intervals
df_t2 = df_t %>% as_tibble() %>%
  mutate(Parameter = gsub("Group", "", Parameter),
         Parameter = gsub("PrePostsacrifice", "BeforeSacrifice", Parameter)); df_t2
```

## Fig. 1C-F
### Supplementary Table 1
```{r}
library(tidyverse) # v.2.0.0
library(ggbreak) # 0.1.0
library(patchwork) # 1.1.1
set.seed(100)

path = paste0(getwd(), "/Data/DayLv_data.tsv")
SongRate = read_tsv(path)
SongRate$Group = factor(SongRate$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

SongRate_sum = SongRate %>%
  group_by(Individual) %>%
  summarise(Daily_SongRate_a = mean(Daily_SongRate),
            Daily_SongRate_s = sd(Daily_SongRate)) ; SongRate_sum

path = paste0(getwd(), "/Data/SongLv_data.tsv")
Song_data = read_tsv(path)
Song_data$Group = factor(Song_data$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

Song_data_sum = Song_data %>%
  group_by(Individual) %>%
  summarise(n_song = n(),
            SongLen_a = mean(SongLen),
            SongLen_s = sd(SongLen),
            RepetitionRate_a = mean(RepetitionRate),
            RepetitionRate_s = sd(RepetitionRate),
            SylperSong_a = mean(SylperSong),
            SylperSong_s = sd(SylperSong),
            slope_a = mean(slope),
            slope_s = sd(slope)) ; Song_data_sum

pp = function(StimulatedTable){
  Dayn = 1:nrow(fitmat) # (row of fitmat)
  
  combi = combn(Dayn, 2)
  
  d = lapply(1:ncol(combi), function(combiN){
    # combiN = 1
    tmp = combi[, combiN]
    p = mean(fitmat[tmp[1],]<fitmat[tmp[2],])
    df = tibble("DayN1" = (new_data_mod_a$n_Day[tmp[1]]), 
                "DayN2" = (new_data_mod_a$n_Day[tmp[2]]), "pp" = p)
  })
  
  dd = d %>% bind_rows() %>%
    mutate(`sig (2>1)` = ifelse(pp > 0.95, 'sig', NA))
  return(dd)
}

```


### Fig. 1C, Supplementary Table 2 and 3
```{r}

db = SongRate  

db$Individual = factor(db$Individual)
db$Group = factor(db$Group)
db$n_Group = factor(db$n_Group)
db$n_Day = factor(db$n_Day)
length(unique(db$n_Group))
length(unique(db$n_Day))

db$Individual <- droplevels(db$Individual)
length(unique(db$Individual))
db$Group <- droplevels(db$Group)
length(unique(db$Group))
db$n_Day = droplevels(db$n_Day)
length(unique(db$n_Day))

y_lab = 'Daily singing activity (%)'

## deciding using sqrt or not and evaluating model
mod_a <- lmer(data = db, log(Daily_SongRate) ~ n_Day + (1|Individual) + (1|n_Group) ) # log

plot(mod_a)  
summary(mod_a)

plot.new()
par(mfrow=c(2,2))
qqnorm(resid(mod_a))
qqline(resid(mod_a))
qqnorm(ranef(mod_a)$"Individual"[,1])   # random effect
qqline(ranef(mod_a)$"Individual"[,1])
qqnorm(ranef(mod_a)$"n_Group"[,1])   # random effect
qqline(ranef(mod_a)$"n_Group"[,1])
scatter.smooth(fitted(mod_a), resid(mod_a))


new_data_mod_a <- expand.grid(n_Day = levels(db$n_Day)) # fix effect
xmat <- model.matrix(~ n_Day, data=new_data_mod_a)  # fix effect
new_data_mod_a$fit <- xmat%*%fixef(mod_a)

nsim <- 10000
bsim <- arm::sim(mod_a, n.sim=nsim ) #simulation of the model 10000 times

## To get the mean estimate of the fixed effects parameters apply(bsim@fixef, 2, mean)
apply(bsim@fixef, 2, mean)
## To get the 95% credible Interval estimate of the fixed effects parameters apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))
apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))

Ind_b_CrI = apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975)) 

Ind_b = mod_a@beta
x = as.data.frame(VarCorr(mod_a))
Ind_random_Bird = x[1, 4]
Ind_random_Group = x[2, 4]
Ind_residual = x[3, 4]
Ind_random_Bird_CrI = quantile(apply(bsim@ranef$Individual[,,1],1,var),prob=c(0.025, 0.975))
Ind_random_Group_CrI = quantile(apply(bsim@ranef$n_Group[,,1],1,var),prob=c(0.025, 0.975))
Ind_residual_CrI = quantile(bsim@sigma^2,c(0.025, 0.975))


fitmat <- matrix(ncol=nsim, nrow=nrow(new_data_mod_a))
for(i in 1:nsim) fitmat[,i] <- xmat %*% bsim@fixef[i,]    # fitted values
new_data_mod_a$lower <- apply(fitmat, 1, quantile, prob=0.025)
new_data_mod_a$upper <- apply(fitmat, 1, quantile, prob=0.975)
new_data_mod_a$fit <- apply(fitmat, 1, mean)

new_data_mod_a$fit_exp <- exp(new_data_mod_a$fit)        # transform back from log
new_data_mod_a$upper_exp <- exp(new_data_mod_a$upper)    # transform back from log
new_data_mod_a$lower_exp <- exp(new_data_mod_a$lower)    # transform back from log

w = 0.35
grey_point_size = 0.5
mean_point_size = 1
errorbar_wd = 1
errorbar_hori = 0
dodge = position_jitter(width = 0.05,height = 0)

errorbar_col = "darkorange" # for grey raw data

Daily_SongRate_p = ggplot(new_data_mod_a) +
  # need this layer to set up
  geom_point(data = new_data_mod_a,
             aes(x = n_Day, y = fit_exp), color='black', size=mean_point_size) +
  # raw data
  geom_point(data = db, 
             aes(x = as.numeric(n_Day)+w, y = Daily_SongRate),
             position = dodge, alpha = 0.4, size = grey_point_size) +
  # CI
  geom_errorbar(data = new_data_mod_a,
                aes(x = n_Day, ymax = upper_exp, ymin = lower_exp),
                width = errorbar_hori, size = errorbar_wd,
                color = errorbar_col) +
  # estimate
  geom_point(data = new_data_mod_a,
             aes(x = n_Day, y = fit_exp), color='black', size=mean_point_size) +
  xlab('Day after testosterone implantation') +
  ylab(y_lab) + theme_classic() +
  theme(legend.position = 'none') ; Daily_SongRate_p

```
```{r}
# posterior probabilities
Daily_SongRate_pp = pp(new_data_mod_a); Daily_SongRate_pp
```

```{r}
# estimates and credible intervals
DailySongRate_n_Day_df_t = rbind(cbind("Var" = "DailySongRate", "Parameter" = levels(db$n_Day), 
                   "Estimate" = Ind_b, t(Ind_b_CrI)),
             cbind("Var" = "DailySongRate", "Parameter" = "random_Bird", 
                   "Estimate" =  Ind_random_Bird, t(Ind_random_Bird_CrI)),
             cbind("Var" = "DailySongRate", "Parameter" = "random_Group", 
                   "Estimate" =  Ind_random_Group, t(Ind_random_Group_CrI)),
             cbind("Var" = "DailySongRate", "Parameter" = "residual", 
                   "Estimate" =  Ind_residual, t(Ind_residual_CrI))); DailySongRate_n_Day_df_t

```


### Fig. 1D, Supplementary Table 2 and 3
```{r}

db = Song_data 

head(db)
db$n_Group = factor(db$n_Group)
length(unique(db$n_Group))

db$n_Day = factor(db$n_Day)
db$n_Day = droplevels(db$n_Day)
length(unique(db$n_Day))

db$Individual = factor(db$Individual)
db$Individual <- droplevels(db$Individual)
length(unique(db$Individual))

y_lab = 'Song length (s)'

## deciding using sqrt or not and evaluating model
mod_a <- lmer(data = db, log(SongLen) ~ n_Day + (1|Individual) + (1|n_Group) ) # log

plot(mod_a)  
summary(mod_a)

plot.new()
par(mfrow=c(2,2))
qqnorm(resid(mod_a))
qqline(resid(mod_a))
qqnorm(ranef(mod_a)$"n_Group"[,1])   # random effect
qqline(ranef(mod_a)$"n_Group"[,1])
scatter.smooth(fitted(mod_a), resid(mod_a))



new_data_mod_a <- expand.grid(n_Day = levels(db$n_Day)) # fix effect
xmat <- model.matrix(~ n_Day, data=new_data_mod_a)  # fix effect
new_data_mod_a$fit <- xmat%*%fixef(mod_a)

nsim <- 10000
bsim <- arm::sim(mod_a, n.sim=nsim ) #simulation of the model 10000 times

## To get the mean estimate of the fixed effects parameters apply(bsim@fixef, 2, mean)
apply(bsim@fixef, 2, mean)
## To get the 95% credible Interval estimate of the fixed effects parameters apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))
apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))

Ind_b_CrI = apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975)) 

Ind_b = mod_a@beta
x = as.data.frame(VarCorr(mod_a))
Ind_random_Bird = x[1, 4]
Ind_random_Group = x[2, 4]
Ind_residual = x[3, 4]
Ind_random_Bird_CrI = quantile(apply(bsim@ranef$Individual[,,1],1,var),prob=c(0.025, 0.975))
Ind_random_Group_CrI = quantile(apply(bsim@ranef$n_Group[,,1],1,var),prob=c(0.025, 0.975))
Ind_residual_CrI = quantile(bsim@sigma^2,c(0.025, 0.975))


fitmat <- matrix(ncol=nsim, nrow=nrow(new_data_mod_a))
for(i in 1:nsim) fitmat[,i] <- xmat %*% bsim@fixef[i,]    # fitted values
new_data_mod_a$lower <- apply(fitmat, 1, quantile, prob=0.025)
new_data_mod_a$upper <- apply(fitmat, 1, quantile, prob=0.975)
new_data_mod_a$fit <- apply(fitmat, 1, mean)
new_data_mod_a$fit_exp <- exp(new_data_mod_a$fit)        # transform back from log
new_data_mod_a$upper_exp <- exp(new_data_mod_a$upper)    # transform back from log
new_data_mod_a$lower_exp <- exp(new_data_mod_a$lower)    # transform back from log

w = 0.35
grey_point_size = 0.5
mean_point_size = 1
errorbar_wd = 1
errorbar_hori = 0
dodge = position_jitter(width = 0.05,height = 0)

errorbar_col = "darkorange" # for grey raw data

SongLen_p = ggplot(new_data_mod_a) +
  # need this layer to set up
  geom_point(data = new_data_mod_a,
             aes(x = n_Day, y = fit_exp), color='black', size=mean_point_size) +
  # raw data
  geom_point(data = db, 
             aes(x = as.numeric(n_Day)+w, y = SongLen),
             position = dodge, alpha = 0.4, size = grey_point_size, color = 'grey50') +
  # CI
  geom_errorbar(data = new_data_mod_a,
                aes(x = n_Day, ymax = upper_exp, ymin = lower_exp),
                width = errorbar_hori, size = errorbar_wd,
                color = errorbar_col) +
  # estimate
  geom_point(data = new_data_mod_a,
             aes(x = n_Day, y = fit_exp), color='black', size=mean_point_size) +
  xlab('Day after testosterone implantation') +
  ylab(y_lab) + theme_classic() +
  theme(legend.position = 'none') ; SongLen_p

SongLen_p = SongLen_p +  scale_y_break(c(13, 20), scales = 0.25) +
  theme(axis.title.y = element_text(angle = 90)); SongLen_p

```

```{r}
# posterior probabilities
SongLen_pp = pp(new_data_mod_a); SongLen_pp
```

```{r}
# estimates and credible intervals
SongLen_n_Day_df_t = rbind(cbind("Var" = "SongLen", "Parameter" = levels(db$n_Day), 
                   "Estimate" = Ind_b, t(Ind_b_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "random_Bird", 
                   "Estimate" =  Ind_random_Bird, t(Ind_random_Bird_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "random_Group", 
                   "Estimate" =  Ind_random_Group, t(Ind_random_Group_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "residual", 
                   "Estimate" =  Ind_residual, t(Ind_residual_CrI))); SongLen_n_Day_df_t
```


### Fig. 1E, Supplementary Table 2 and 3
```{r}

db = Song_data 

head(db)
db$n_Group = factor(db$n_Group)
length(unique(db$n_Group))

db$n_Day = factor(db$n_Day)
db$n_Day = droplevels(db$n_Day)
length(unique(db$n_Day))

db$Individual = factor(db$Individual)
db$Individual <- droplevels(db$Individual)
length(unique(db$Individual))

y_lab = 'Number of syllables per song'

## deciding using sqrt or not and evaluating model
mod_a <- lmer(data = db, log(SylperSong) ~ n_Day + (1|Individual) + (1|n_Group) ) # log

plot(mod_a)  
summary(mod_a)

plot.new()
par(mfrow=c(2,2))
qqnorm(resid(mod_a))
qqline(resid(mod_a))
qqnorm(ranef(mod_a)$"Individual"[,1])   # random effect
qqline(ranef(mod_a)$"Individual"[,1])
qqnorm(ranef(mod_a)$"n_Group"[,1])   # random effect
qqline(ranef(mod_a)$"n_Group"[,1])
scatter.smooth(fitted(mod_a), resid(mod_a))



new_data_mod_a <- expand.grid(n_Day = levels(db$n_Day)) # fix effect
xmat <- model.matrix(~ n_Day, data=new_data_mod_a)  # fix effect
new_data_mod_a$fit <- xmat%*%fixef(mod_a)

nsim <- 10000
bsim <- arm::sim(mod_a, n.sim=nsim ) #simulation of the model 10000 times

## To get the mean estimate of the fixed effects parameters apply(bsim@fixef, 2, mean)
apply(bsim@fixef, 2, mean)
## To get the 95% credible Interval estimate of the fixed effects parameters apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))
apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))

Ind_b_CrI = apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975)) 

Ind_b = mod_a@beta
x = as.data.frame(VarCorr(mod_a))
Ind_random_Bird = x[1, 4]
Ind_random_Group = x[2, 4]
Ind_residual = x[3, 4]
Ind_random_Bird_CrI = quantile(apply(bsim@ranef$Individual[,,1],1,var),prob=c(0.025, 0.975))
Ind_random_Group_CrI = quantile(apply(bsim@ranef$n_Group[,,1],1,var),prob=c(0.025, 0.975))
Ind_residual_CrI = quantile(bsim@sigma^2,c(0.025, 0.975))


fitmat <- matrix(ncol=nsim, nrow=nrow(new_data_mod_a))
for(i in 1:nsim) fitmat[,i] <- xmat %*% bsim@fixef[i,]    # fitted values
new_data_mod_a$lower <- apply(fitmat, 1, quantile, prob=0.025)
new_data_mod_a$upper <- apply(fitmat, 1, quantile, prob=0.975)
new_data_mod_a$fit <- apply(fitmat, 1, mean)

new_data_mod_a$fit_exp <- exp(new_data_mod_a$fit)        # transform back from log
new_data_mod_a$upper_exp <- exp(new_data_mod_a$upper)    # transform back from log
new_data_mod_a$lower_exp <- exp(new_data_mod_a$lower)    # transform back from log

w = 0.35
grey_point_size = 0.5
mean_point_size = 1
errorbar_wd = 1
errorbar_hori = 0
dodge = position_jitter(width = 0.05,height = 0)

errorbar_col = "darkorange" # for grey raw data

SylperSong_p = ggplot(new_data_mod_a) +
  # need this layer to set up
  geom_point(data = new_data_mod_a,
             aes(x = n_Day, y = fit_exp), color='black', size=mean_point_size) +
  # raw data
  geom_point(data = db, 
             aes(x = as.numeric(n_Day)+w, y = SylperSong),
             position = dodge, alpha = 0.4, size = grey_point_size, color = 'grey50') +
  # CI
  geom_errorbar(data = new_data_mod_a,
                aes(x = n_Day, ymax = upper_exp, ymin = lower_exp),
                width = errorbar_hori, size = errorbar_wd,
                color = errorbar_col) +
  # estimate
  geom_point(data = new_data_mod_a,
             aes(x = n_Day, y = fit_exp), color='black', size=mean_point_size) +
  xlab('Day after testosterone implantation') +
  ylab(y_lab) + theme_classic() +
  theme(legend.position = 'none') ; SylperSong_p

SylperSong_p = SylperSong_p +  scale_y_break(c(90, 200), scales = 0.25) +
  theme(axis.title.y = element_text(angle = 90)); SylperSong_p

```

```{r}
# posterior probabilities
SylperSong_pp = pp(new_data_mod_a); SylperSong_pp
```

```{r}
# estimates and credible intervals
SylperSong_n_Day_df_t = rbind(cbind("Var" = "SongLen", "Parameter" = levels(db$n_Day), 
                   "Estimate" = Ind_b, t(Ind_b_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "random_Bird", 
                   "Estimate" =  Ind_random_Bird, t(Ind_random_Bird_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "random_Group", 
                   "Estimate" =  Ind_random_Group, t(Ind_random_Group_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "residual", 
                   "Estimate" =  Ind_residual, t(Ind_residual_CrI))); SylperSong_n_Day_df_t
```

### Fig. 1F, Supplementary Table 2 and 3
```{r}

db = Song_data 

head(db)
db$n_Group = factor(db$n_Group)
length(unique(db$n_Group))

db$n_Day = factor(db$n_Day)
db$n_Day = droplevels(db$n_Day)
length(unique(db$n_Day))

db$Individual = factor(db$Individual)
db$Individual <- droplevels(db$Individual)
length(unique(db$Individual))

y_lab = 'Repetition rate (syl/s)'

## deciding using sqrt or not and evaluating model
mod_a <- lmer(data = db, log(RepetitionRate) ~ n_Day + (1|Individual) + (1|n_Group) ) # log

plot(mod_a)  
summary(mod_a)

plot.new()
par(mfrow=c(2,2))
qqnorm(resid(mod_a))
qqline(resid(mod_a))
qqnorm(ranef(mod_a)$"Individual"[,1])   # random effect
qqline(ranef(mod_a)$"Individual"[,1])
qqnorm(ranef(mod_a)$"n_Group"[,1])   # random effect
qqline(ranef(mod_a)$"n_Group"[,1])
scatter.smooth(fitted(mod_a), resid(mod_a))



new_data_mod_a <- expand.grid(n_Day = levels(db$n_Day)) # fix effect
xmat <- model.matrix(~ n_Day, data=new_data_mod_a)  # fix effect
new_data_mod_a$fit <- xmat%*%fixef(mod_a)

nsim <- 10000
bsim <- arm::sim(mod_a, n.sim=nsim ) #simulation of the model 10000 times

## To get the mean estimate of the fixed effects parameters apply(bsim@fixef, 2, mean)
apply(bsim@fixef, 2, mean)
## To get the 95% credible Interval estimate of the fixed effects parameters apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))
apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))

Ind_b_CrI = apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975)) 

Ind_b = mod_a@beta
x = as.data.frame(VarCorr(mod_a))
Ind_random_Bird = x[1, 4]
Ind_random_Group = x[2, 4]
Ind_residual = x[3, 4]
Ind_random_Bird_CrI = quantile(apply(bsim@ranef$Individual[,,1],1,var),prob=c(0.025, 0.975))
Ind_random_Group_CrI = quantile(apply(bsim@ranef$n_Group[,,1],1,var),prob=c(0.025, 0.975))
Ind_residual_CrI = quantile(bsim@sigma^2,c(0.025, 0.975))


fitmat <- matrix(ncol=nsim, nrow=nrow(new_data_mod_a))
for(i in 1:nsim) fitmat[,i] <- xmat %*% bsim@fixef[i,]    # fitted values
new_data_mod_a$lower <- apply(fitmat, 1, quantile, prob=0.025)
new_data_mod_a$upper <- apply(fitmat, 1, quantile, prob=0.975)
new_data_mod_a$fit <- apply(fitmat, 1, mean)

new_data_mod_a$fit_exp <- exp(new_data_mod_a$fit)        # transform back from log
new_data_mod_a$upper_exp <- exp(new_data_mod_a$upper)    # transform back from log
new_data_mod_a$lower_exp <- exp(new_data_mod_a$lower)    # transform back from log

w = 0.35
grey_point_size = 0.5
mean_point_size = 1
errorbar_wd = 1
errorbar_hori = 0
dodge = position_jitter(width = 0.05,height = 0)

errorbar_col = "darkorange" # for grey raw data

RepetitionRate_p = ggplot(new_data_mod_a) +
  # need this layer to set up
  geom_point(data = new_data_mod_a,
             aes(x = n_Day, y = fit_exp), color='black', size=mean_point_size) +
  # raw data
  geom_point(data = db, 
             aes(x = as.numeric(n_Day)+w, y = RepetitionRate),
             position = dodge, alpha = 0.4, size = grey_point_size, color = 'grey50') +
  # CI
  geom_errorbar(data = new_data_mod_a,
                aes(x = n_Day, ymax = upper_exp, ymin = lower_exp),
                width = errorbar_hori, size = errorbar_wd,
                color = errorbar_col) +
  # estimate
  geom_point(data = new_data_mod_a,
             aes(x = n_Day, y = fit_exp), color='black', size=mean_point_size) +
  xlab('Day after testosterone implantation') +
  ylab(y_lab) + theme_classic() +
  theme(legend.position = 'none') ; RepetitionRate_p

RepetitionRate_p = RepetitionRate_p + 
  scale_y_break(c(10, 20), scales = 0.25) +
  theme(axis.title.y = element_text(angle = 90)); RepetitionRate_p
```

```{r}
# posterior probabilities
RepetitionRate_pp = pp(new_data_mod_a); RepetitionRate_pp
```

```{r}
# estimates and credible intervals
RepetitionRate_n_Day_df_t = rbind(cbind("Var" = "SongLen", "Parameter" = levels(db$n_Day), 
                   "Estimate" = Ind_b, t(Ind_b_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "random_Bird", 
                   "Estimate" =  Ind_random_Bird, t(Ind_random_Bird_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "random_Group", 
                   "Estimate" =  Ind_random_Group, t(Ind_random_Group_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "residual", 
                   "Estimate" =  Ind_residual, t(Ind_residual_CrI))); RepetitionRate_n_Day_df_t
```

### Fig. 1G
```{r}
library(tidyverse) # v.2.0.0
library(PMCMRplus)

path = paste0(getwd(), "/Data/HVC_volume.tsv")
HVC = read_tsv(path)
HVC$Group = factor(HVC$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

HVC %>% group_by(Group) %>% summarise( count = length(unique(ProcessNum)),
                                         mean = mean(`HVC Volume (mm3)`, na.rm = T), 
                                         sd = sd(`HVC Volume (mm3)`, na.rm = T))
```

```{r}
w = 0.25
HVCb = ggplot(HVC, aes(y = `HVC Volume (mm3)`, x = Group)) + 
  geom_boxplot( outlier.colour = 'red', outlier.size = 0.5, width = 0.2) + 
  geom_point( position = position_nudge(x = +w), size = 0.25) + 
  scale_color_manual(values = rep("black", 2)) +
  scale_y_continuous(name = expression(HVC*" "*volume*" ("*mm^3*")")) + 
  theme_classic(); HVCb
```

```{r}
# The Kruskal???Wallis test (One-way ANOVA on ranks)
kruskal.test(`HVC Volume (mm3)` ~ Group, data = HVC) 
# Kruskal-Wallis chi-squared = 16.467, df = 6, p-value = 0.01145 

# post-hoc
kwAllPairsDunnTest(`HVC Volume (mm3)` ~ Group, data = HVC, p.adjust.method = "holm")
#       CON   T1h   T3h   T8h   T3d   T7d  
# T1h  1.000 -     -     -     -     -    
# T3h  1.000 1.000 -     -     -     -    
# T8h  1.000 1.000 1.000 -     -     -    
# T3d  1.000 1.000 1.000 1.000 -     -    
# T7d  1.000 1.000 1.000 1.000 1.000 -    
# T14d 0.027 0.014 0.425 0.030 0.224 0.815
```

### Fig. 1 - Figure supplement 2A
```{r}
library(tidyverse) # v.2.0.0
set.seed(100)

path = paste0(getwd(), "/Data/DayLv_data.tsv")
SongRate = read_tsv(path)
SongRate$Group = factor(SongRate$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

db = SongRate 
db$n_Group = factor(db$n_Group)
db$n_Day = factor(db$n_Day)

# show individual
db$Ind = gsub('b', "bird ", db$Ind) 
db$Ind = factor(db$Ind, levels = paste0('bird ', 1:12))

y_lab = 'Daily singing activity (%)'

w = 0.35
grey_point_size = 0.5
mean_point_size = 1
errorbar_wd = 1
errorbar_hori = 0
dodge = position_jitter(width = 0.05,height = 0)

Daily_SongRate_p = ggplot(data = db, aes(x = as.numeric(n_Day), y = Daily_SongRate,
                                    color = as.factor(Ind))) +
  # raw data
  geom_point(position = dodge, alpha = 0.5, size = grey_point_size) +
  scale_x_continuous(breaks = seq(0, 14, by = 2), limits = c(NA, 14)) +
  facet_wrap(.~Ind, drop = FALSE, nrow = 2 ) +
  xlab('Day after testosterone implantation') +
  ylab(y_lab) +
  guides(color = guide_legend(ncol=3)) +
  theme_classic() + 
  theme(panel.border = element_rect(size = 0.2, fill = 'transparent'),
        legend.position = 'none'); Daily_SongRate_p
```


### Fig. 1 - Figure supplement 2B
```{r}
library(tidyverse) # v.2.0.0
set.seed(100)

path = paste0(getwd(), "/Data/SongLv_data.tsv")
Song_data = read_tsv(path)
Song_data$Group = factor(Song_data$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

db = Song_data 

db$n_Group = factor(db$n_Group)
db$n_Day = factor(db$n_Day)

# show individual
db$Ind = gsub('b', "bird ", db$Ind)
db$Ind = factor(db$Ind, levels = paste0('bird ', 1:12))

y_lab = 'Song length (s)'

SongLen_p = ggplot(db) +
  # raw data
  geom_point(aes(x = as.numeric(n_Day), y = SongLen,
                 color = as.factor(Ind)),
             position = dodge, alpha = 0.5, size = grey_point_size) +
  # scale_colour_manual(values = col_palette) +
  scale_x_continuous(breaks = seq(0, 14, by = 2), limits = c(NA, 14)) +
  facet_wrap(.~Ind, drop = FALSE, nrow = 2 ) +
  xlab('Day after testosterone implantation') +
  ylab(y_lab) +
  theme_classic() + 
  theme(panel.border = element_rect(size = 0.2, fill = 'transparent'),
        legend.position = 'none'); SongLen_p
```


### Fig. 1 - Figure supplement 2C
```{r}
library(tidyverse) # v.2.0.0
set.seed(100)

path = paste0(getwd(), "/Data/SongLv_data.tsv")
Song_data = read_tsv(path)
Song_data$Group = factor(Song_data$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

db = Song_data 

db$n_Group = factor(db$n_Group)
db$n_Day = factor(db$n_Day)

# show individual
db$Ind = gsub('b', "bird ", db$Ind)
db$Ind = factor(db$Ind, levels = paste0('bird ', 1:12))

y_lab = 'Repetition rate (syl/s)'

RepetitionRate_p = ggplot(db, aes(x = as.numeric(n_Day), y = RepetitionRate,
                             color = as.factor(Ind))) +
  # raw data
  geom_point(position = dodge, alpha = 0.5, size = grey_point_size) +
  # scale_colour_manual(values = col_palette) +
  scale_x_continuous(breaks = seq(0, 14, by = 2), limits = c(NA, 14)) +
  facet_wrap(.~Ind, drop = FALSE, nrow = 2 ) +
  xlab('Day after testosterone implantation') +
  ylab(y_lab) +
  theme_classic() + 
  theme(panel.border = element_rect(size = 0.2, fill = 'transparent'),
        legend.position = 'none'); RepetitionRate_p

```


### Fig. 1 - Figure supplement 2D
```{r}
library(tidyverse) # v.2.0.0
set.seed(100)

path = paste0(getwd(), "/Data/SongLv_data.tsv")
Song_data = read_tsv(path)
Song_data$Group = factor(Song_data$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

db = Song_data 

db$n_Group = factor(db$n_Group)
db$n_Day = factor(db$n_Day)

# show individual
db$Ind = gsub('b', "bird ", db$Ind)
db$Ind = factor(db$Ind, levels = paste0('bird ', 1:12))

y_lab = 'Number of syllables per song'

SylperSong_p = ggplot(db, aes(x = as.numeric(n_Day), y = SylperSong,
                           color = as.factor(Ind))) +
  # raw data
  geom_point(position = dodge, alpha = 0.5, size = grey_point_size) +
  # scale_colour_manual(values = col_palette) +
  scale_x_continuous(breaks = seq(0, 14, by = 2), limits = c(NA, 14)) +
  facet_wrap(.~Ind, drop = FALSE, nrow = 2 ) +
  xlab('Day after testosterone implantation') +
  ylab(y_lab) +
  theme_classic() + 
  theme(panel.border = element_rect(size = 0.2, fill = 'transparent'),
        legend.position = 'none'); SylperSong_p

```

### Fig. 1 - Figure supplement 3A, Supplementary Table 2 and 3
```{r}
library(tidyverse) # v.2.0.0
set.seed(100)

path = paste0(getwd(), "/Data/DayLv_data.tsv")
SongRate = read_tsv(path)
SongRate$Group = factor(SongRate$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

db = SongRate %>% 
  filter(n_Day <= 7) 

db$Individual <- factor(db$Individual)
db$Individual <- droplevels(db$Individual)
db$n_Group = factor(db$n_Group)
db$Group <- droplevels(db$Group)

y_lab = 'Daily singing activity (%)'

# 1. deciding using sqrt or not and evaluating model
rm(mod_a)
mod_a <- lmer(data = db, log(Daily_SongRate) ~ Group + (1|Individual) ) # log

plot(mod_a)  
summary(mod_a)

plot.new()
par(mfrow=c(3,2))
qqnorm(resid(mod_a))
qqline(resid(mod_a))
qqnorm(ranef(mod_a)$"Individual"[,1])   # random effect
qqline(ranef(mod_a)$"Individual"[,1])
scatter.smooth(fitted(mod_a), resid(mod_a))

new_data_mod_a <- expand.grid(Group = levels(db$Group)) # fix effect
xmat <- model.matrix(~ Group, data=new_data_mod_a)  # fix effect
new_data_mod_a$fit <- xmat%*%fixef(mod_a)

nsim <- 10000
bsim <- arm::sim(mod_a, n.sim=nsim ) #simulation of the model 10000 times
## To get the mean estimate of the fixed effects parameters apply(bsim@fixef, 2, mean)
apply(bsim@fixef, 2, mean)
## To get the 95% credible Interval estimate of the fixed effects parameters apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))
apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))

Ind_b_CrI = apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975)) 

Ind_b = mod_a@beta
x = as.data.frame(VarCorr(mod_a))
Ind_random = x[1, 4]
Ind_residual = x[2, 4]
Ind_random_CrI = quantile(apply(bsim@ranef$Individual[,,1],1,var),prob=c(0.025, 0.975))
Ind_residual_CrI = quantile(bsim@sigma^2,c(0.025, 0.975))

fitmat <- matrix(ncol=nsim, nrow=nrow(new_data_mod_a))
for(i in 1:nsim) fitmat[,i] <- xmat %*% bsim@fixef[i,]    # fitted values
new_data_mod_a$lower <- apply(fitmat, 1, quantile, prob=0.025)
new_data_mod_a$upper <- apply(fitmat, 1, quantile, prob=0.975)
new_data_mod_a$fit <- apply(fitmat, 1, mean)
new_data_mod_a$fit_exp <- exp(new_data_mod_a$fit)        # transform back from log
new_data_mod_a$upper_exp <- exp(new_data_mod_a$upper)    # transform back from log
new_data_mod_a$lower_exp <- exp(new_data_mod_a$lower)    # transform back from log

w = 0.25
grey_point_size = 0.2
mean_point_size = 1
errorbar_wd = 1
errorbar_hori = 0
dodge = position_jitter(width = 0.1, height = 0)

errorbar_col = "darkorange" # for grey raw data

Daily_SongRate_Grp_p = ggplot(new_data_mod_a) +
  # need this layer to set up
  geom_point(data = new_data_mod_a,
             aes(x = Group, y = fit_exp), color='black', size=mean_point_size) +
  # raw data
  geom_point(data = db, 
             aes(x = as.numeric(Group)+w, y = Daily_SongRate),
             position = dodge, alpha = 0.5, size = grey_point_size, color = "grey50") +
  # CI
  geom_errorbar(data = new_data_mod_a,
                aes(x = Group, ymax = upper_exp, ymin = lower_exp),
                width = errorbar_hori, size = errorbar_wd,
                color = errorbar_col) +
  # estimate
  geom_point(data = new_data_mod_a,
             aes(x = Group, y = fit_exp), color='black', size=mean_point_size) +
  xlab('Group') +
  ylab(y_lab) +
  theme_classic() ; Daily_SongRate_Grp_p

```

```{r}
# posterior probabilities
Daily_SongRate_Grp_pp = pp(new_data_mod_a)

# estimates and credible intervals
Daily_SongRate_Grp_df_t = rbind(cbind("Var" = "Daily_SongRate", "Parameter" = levels(db$Group), 
                   "Estimate" = Ind_b, t(Ind_b_CrI)),
             cbind("Var" = "Daily_SongRate", "Parameter" = "random", 
                   "Estimate" =  Ind_random, t(Ind_random_CrI)),
             cbind("Var" = "Daily_SongRate", "Parameter" = "residual", 
                   "Estimate" =  Ind_residual, t(Ind_residual_CrI)))
```



### Fig. 1 - Figure supplement 3B, Supplementary Table 2 and 3
```{r}
library(tidyverse) # v.2.0.0
set.seed(100)

path = paste0(getwd(), "/Data/SongLv_data.tsv")
Song_data = read_tsv(path)
Song_data$Group = factor(Song_data$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

db = Song_data %>% 
  filter(n_Day <= 7)
  
db$Group = factor(db$Group, levels = c("T7d", "T14d"))
db$Individual <- factor(db$Individual)
db$Individual <- droplevels(db$Individual)

y_lab = 'Song length (s)'

# 1. deciding using sqrt or not and evaluating model
rm(mod_a)
mod_a <- lmer(data = db, log(SongLen) ~ Group + (1|Individual) ) # log

plot(mod_a)  
summary(mod_a)

plot.new()
par(mfrow=c(3,2))
qqnorm(resid(mod_a))
qqline(resid(mod_a))
qqnorm(ranef(mod_a)$"Individual"[,1])   # random effect
qqline(ranef(mod_a)$"Individual"[,1])
scatter.smooth(fitted(mod_a), resid(mod_a))

new_data_mod_a <- expand.grid(Group = levels(db$Group)) # fix effect
xmat <- model.matrix(~ Group, data=new_data_mod_a)  # fix effect
new_data_mod_a$fit <- xmat%*%fixef(mod_a)

nsim <- 10000
bsim <- arm::sim(mod_a, n.sim=nsim ) #simulation of the model 10000 times
## To get the mean estimate of the fixed effects parameters apply(bsim@fixef, 2, mean)
apply(bsim@fixef, 2, mean)
## To get the 95% credible Interval estimate of the fixed effects parameters apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))
apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))

Ind_b_CrI = apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975)) 

Ind_b = mod_a@beta
x = as.data.frame(VarCorr(mod_a))
Ind_random = x[1, 4]
Ind_residual = x[2, 4]
Ind_random_CrI = quantile(apply(bsim@ranef$Individual[,,1],1,var),prob=c(0.025, 0.975))
Ind_residual_CrI = quantile(bsim@sigma^2,c(0.025, 0.975))

fitmat <- matrix(ncol=nsim, nrow=nrow(new_data_mod_a))
for(i in 1:nsim) fitmat[,i] <- xmat %*% bsim@fixef[i,]    # fitted values
new_data_mod_a$lower <- apply(fitmat, 1, quantile, prob=0.025)
new_data_mod_a$upper <- apply(fitmat, 1, quantile, prob=0.975)
new_data_mod_a$fit <- apply(fitmat, 1, mean)
new_data_mod_a$fit_exp <- exp(new_data_mod_a$fit)        # transform back from log
new_data_mod_a$upper_exp <- exp(new_data_mod_a$upper)    # transform back from log
new_data_mod_a$lower_exp <- exp(new_data_mod_a$lower)    # transform back from log

w = 0.25
grey_point_size = 0.2
mean_point_size = 1
errorbar_wd = 1
errorbar_hori = 0
dodge = position_jitter(width = 0.1, height = 0)

errorbar_col = "darkorange" # for grey raw data

SongLen_Grp_p = ggplot(new_data_mod_a) +
  # need this layer to set up
  geom_point(data = new_data_mod_a,
             aes(x = Group, y = fit_exp), color='black', size=mean_point_size) +
  # raw data
  geom_point(data = db, 
             aes(x = as.numeric(Group)+w, y = SongLen),
             position = dodge, alpha = 0.1, size = grey_point_size, color = "grey50") +
  # CI
  geom_errorbar(data = new_data_mod_a,
                aes(x = Group, ymax = upper_exp, ymin = lower_exp),
                width = errorbar_hori, size = errorbar_wd,
                color = errorbar_col) +
  # estimate
  geom_point(data = new_data_mod_a,
             aes(x = Group, y = fit_exp), color='black', size=mean_point_size) +
  xlab('Group') +
  ylab(y_lab) +
  theme_classic() ; SongLen_Grp_p

```

```{r}
# posterior probabilities
SongLen_Grp_pp = pp(new_data_mod_a)

# estimates and credible intervals
SongLen_Grp_df_t = rbind(cbind("Var" = "SongLen", "Parameter" = levels(db$Group), 
                   "Estimate" = Ind_b, t(Ind_b_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "random", 
                   "Estimate" =  Ind_random, t(Ind_random_CrI)),
             cbind("Var" = "SongLen", "Parameter" = "residual", 
                   "Estimate" =  Ind_residual, t(Ind_residual_CrI)))


```



### Fig. 1 - Figure supplement 3C, Supplementary Table 2 and 3
```{r}
library(tidyverse) # v.2.0.0
set.seed(100)

path = paste0(getwd(), "/Data/SongLv_data.tsv")
Song_data = read_tsv(path)
Song_data$Group = factor(Song_data$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

db = Song_data %>% 
  filter(n_Day <= 7)

db$Individual <- factor(db$Individual)
db$Individual <- droplevels(db$Individual)
db$Group = factor(db$Group, levels = c("T7d", "T14d"))

y_lab = 'Repetition rate (syl/s)'

# 1. deciding using sqrt or not and evaluating model
rm(mod_a)
mod_a <- lmer(data = db, log(RepetitionRate) ~ Group + (1|Individual) ) # log

plot(mod_a)  
summary(mod_a)

plot.new()
par(mfrow=c(3,2))
qqnorm(resid(mod_a))
qqline(resid(mod_a))
qqnorm(ranef(mod_a)$"Individual"[,1])   # random effect
qqline(ranef(mod_a)$"Individual"[,1])
scatter.smooth(fitted(mod_a), resid(mod_a))

new_data_mod_a <- expand.grid(Group = levels(db$Group)) # fix effect
xmat <- model.matrix(~ Group, data=new_data_mod_a)  # fix effect
new_data_mod_a$fit <- xmat%*%fixef(mod_a)

nsim <- 10000
bsim <- arm::sim(mod_a, n.sim=nsim ) #simulation of the model 10000 times
## To get the mean estimate of the fixed effects parameters apply(bsim@fixef, 2, mean)
apply(bsim@fixef, 2, mean)
## To get the 95% credible Interval estimate of the fixed effects parameters apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))
apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))

Ind_b_CrI = apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975)) 

Ind_b = mod_a@beta
x = as.data.frame(VarCorr(mod_a))
Ind_random = x[1, 4]
Ind_residual = x[2, 4]
Ind_random_CrI = quantile(apply(bsim@ranef$Individual[,,1],1,var),prob=c(0.025, 0.975))
Ind_residual_CrI = quantile(bsim@sigma^2,c(0.025, 0.975))

# quantile(apply(bsim@ranef$lugar[,,1],1,var),prob=c(0.025, 0.5, 0.975))
# quantile(bsim@sigma^2,c(0.025,0.5,0.975))

fitmat <- matrix(ncol=nsim, nrow=nrow(new_data_mod_a))
for(i in 1:nsim) fitmat[,i] <- xmat %*% bsim@fixef[i,]    # fitted values
new_data_mod_a$lower <- apply(fitmat, 1, quantile, prob=0.025)
new_data_mod_a$upper <- apply(fitmat, 1, quantile, prob=0.975)
new_data_mod_a$fit <- apply(fitmat, 1, mean)
new_data_mod_a$fit_exp <- exp(new_data_mod_a$fit)        # transform back from log
new_data_mod_a$upper_exp <- exp(new_data_mod_a$upper)    # transform back from log
new_data_mod_a$lower_exp <- exp(new_data_mod_a$lower)    # transform back from log

w = 0.25
grey_point_size = 0.2
mean_point_size = 1
errorbar_wd = 1
errorbar_hori = 0
dodge = position_jitter(width = 0.1, height = 0)

errorbar_col = "darkorange" # for grey raw data

RepetitionRate_Grp_p = ggplot(new_data_mod_a) +
  # need this layer to set up
  geom_point(data = new_data_mod_a,
             aes(x = Group, y = fit_exp), color='black', size=mean_point_size) +
  # raw data
  geom_point(data = db, 
             aes(x = as.numeric(Group)+w, y = RepetitionRate),
             position = dodge, alpha = 0.1, size = grey_point_size, color = "grey50") +
  # CI
  geom_errorbar(data = new_data_mod_a,
                aes(x = Group, ymax = upper_exp, ymin = lower_exp),
                width = errorbar_hori, size = errorbar_wd,
                color = errorbar_col) +
  # estimate
  geom_point(data = new_data_mod_a,
             aes(x = Group, y = fit_exp), color='black', size=mean_point_size) +
  xlab('Group') +
  ylab(y_lab) +
  theme_classic() ; RepetitionRate_Grp_p

```


```{r}
# posterior probabilities
RepetitionRate_Grp_pp = pp(new_data_mod_a)

# estimates and credible intervals
RepetitionRate_Grp_df_t = rbind(cbind("Var" = "RepetitionRate", "Parameter" = levels(db$Group), 
                   "Estimate" = Ind_b, t(Ind_b_CrI)),
             cbind("Var" = "RepetitionRate", "Parameter" = "random", 
                   "Estimate" =  Ind_random, t(Ind_random_CrI)),
             cbind("Var" = "RepetitionRate", "Parameter" = "residual", 
                   "Estimate" =  Ind_residual, t(Ind_residual_CrI)))

```

### Fig. 1 - Figure supplement 3D, Supplementary Table 2 and 3
```{r}
library(tidyverse) # v.2.0.0
set.seed(100)

path = paste0(getwd(), "/Data/SongLv_data.tsv")
Song_data = read_tsv(path)
Song_data$Group = factor(Song_data$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

db = Song_data %>% 
  filter(n_Day <= 7)

db$Individual <- factor(db$Individual)
db$Individual <- droplevels(db$Individual)

db$Group = factor(db$Group, levels = c("T7d", "T14d"))

y_lab = 'Number of syllables per song'

# 1. deciding using sqrt or not and evaluating model
rm(mod_a)
mod_a <- lmer(data = db, log(SylperSong) ~ Group + (1|Individual) ) # log

plot(mod_a)  
summary(mod_a)

plot.new()
par(mfrow=c(3,2))
qqnorm(resid(mod_a))
qqline(resid(mod_a))
qqnorm(ranef(mod_a)$"Individual"[,1])   # random effect
qqline(ranef(mod_a)$"Individual"[,1])
scatter.smooth(fitted(mod_a), resid(mod_a))

new_data_mod_a <- expand.grid(Group = levels(db$Group)) # fix effect
xmat <- model.matrix(~ Group, data=new_data_mod_a)  # fix effect
new_data_mod_a$fit <- xmat%*%fixef(mod_a)

nsim <- 10000
bsim <- arm::sim(mod_a, n.sim=nsim ) #simulation of the model 10000 times
## To get the mean estimate of the fixed effects parameters apply(bsim@fixef, 2, mean)
apply(bsim@fixef, 2, mean)
## To get the 95% credible Interval estimate of the fixed effects parameters apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))
apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975))

Ind_b_CrI = apply(bsim@fixef, 2, quantile, prob=c(0.025, 0.975)) 

Ind_b = mod_a@beta
x = as.data.frame(VarCorr(mod_a))
Ind_random = x[1, 4]
Ind_residual = x[2, 4]
Ind_random_CrI = quantile(apply(bsim@ranef$Individual[,,1],1,var),prob=c(0.025, 0.975))
Ind_residual_CrI = quantile(bsim@sigma^2,c(0.025, 0.975))

# quantile(apply(bsim@ranef$lugar[,,1],1,var),prob=c(0.025, 0.5, 0.975))
# quantile(bsim@sigma^2,c(0.025,0.5,0.975))

fitmat <- matrix(ncol=nsim, nrow=nrow(new_data_mod_a))
for(i in 1:nsim) fitmat[,i] <- xmat %*% bsim@fixef[i,]    # fitted values
new_data_mod_a$lower <- apply(fitmat, 1, quantile, prob=0.025)
new_data_mod_a$upper <- apply(fitmat, 1, quantile, prob=0.975)
new_data_mod_a$fit <- apply(fitmat, 1, mean)
new_data_mod_a$fit_exp <- exp(new_data_mod_a$fit)        # transform back from log
new_data_mod_a$upper_exp <- exp(new_data_mod_a$upper)    # transform back from log
new_data_mod_a$lower_exp <- exp(new_data_mod_a$lower)    # transform back from log

w = 0.25
grey_point_size = 0.2
mean_point_size = 1
errorbar_wd = 1
errorbar_hori = 0
dodge = position_jitter(width = 0.1, height = 0)

errorbar_col = "darkorange" # for grey raw data

SylperSong_Grp_p = ggplot(new_data_mod_a) +
  # need this layer to set up
  geom_point(data = new_data_mod_a,
             aes(x = Group, y = fit_exp), color='black', size=mean_point_size) +
  # raw data
  geom_point(data = db, 
             aes(x = as.numeric(Group)+w, y = SylperSong),
             position = dodge, alpha = 0.1, size = grey_point_size, color = 'grey50') +
  # CI
  geom_errorbar(data = new_data_mod_a,
                aes(x = Group, ymax = upper_exp, ymin = lower_exp),
                width = errorbar_hori, size = errorbar_wd,
                color = errorbar_col) +
  # estimate
  geom_point(data = new_data_mod_a,
             aes(x = Group, y = fit_exp), color='black', size=mean_point_size) +
  xlab('Group') +
  ylab(y_lab) +
  theme_classic() ; SylperSong_Grp_p

```


```{r}
# posterior probabilities
SylperSong_Grp_pp = pp(new_data_mod_a)
# estimates and credible intervals
SylperSong_Grp_df_t = rbind(cbind("Var" = "SylperSong", "Parameter" = levels(db$Group), 
                   "Estimate" = Ind_b, t(Ind_b_CrI)),
             cbind("Var" = "SylperSong", "Parameter" = "random", 
                   "Estimate" =  Ind_random, t(Ind_random_CrI)),
             cbind("Var" = "SylperSong", "Parameter" = "residual", 
                   "Estimate" =  Ind_residual, t(Ind_residual_CrI)))
```




### Fig. 1 - Figure supplement 4A
```{r}
library(tidyverse) # v.2.0.0
library(PMCMRplus) # 1.9.4
path =  paste0(getwd(), "/Data/Physiological_measurements.tsv")
Weights = read_tsv(path)
Weights$Group <- factor(Weights$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

w = 0.25
BodyWeightb = ggplot(Weights, aes(y = `Body Weight (g)`, x = Group)) + 
  geom_boxplot( outlier.colour = 'red', outlier.size = 0.5, width = 0.2) + 
  geom_point( position = position_nudge(x = w), size = 0.25) + 
  scale_color_manual(values = rep("black", 2)) +
  scale_y_continuous(name = "Body weight (g)" ) +
  theme_classic() ; BodyWeightb

# The Kruskal???Wallis test (One-way ANOVA on ranks)
kruskal.test(`Body Weight (g)` ~ Group, data = Weights) 
# Kruskal-Wallis chi-squared = 7.3905, df = 6, p-value = 0.2862
```

### Fig. 1 - Figure supplement 4B
```{r}
library(tidyverse) # v.2.0.0
library(PMCMRplus) # 1.9.4
path =  paste0(getwd(), "/Data/Physiological_measurements.tsv")
Weights = read_tsv(path)
Weights$Group <- factor(Weights$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

w = 0.25
BrainWeightb = ggplot(Weights, aes(y = `Brain (mg)`, x = Group)) + 
  geom_boxplot( outlier.colour = 'red', outlier.size = 0.5, width = 0.2) + 
  geom_point( position = position_nudge(x = w), size = 0.25) + 
  scale_color_manual(values = rep("black", 2)) +
  scale_y_continuous(name = "Brain weight (mg)") +
  theme_classic() ; BrainWeightb

# The Kruskal???Wallis test (One-way ANOVA on ranks)
kruskal.test(`Brain (mg)` ~ Group, data = Weights) 
# Kruskal-Wallis chi-squared = 15.704, df = 6, p-value = 0.01543


kwAllPairsDunnTest(`Brain (mg)` ~ Group, data = Weights, p.adjust.method="holm")
#      CON   T1h   T3h   T8h   T3d   T7d  
# T1h  0.080 -     -     -     -     -    
# T3h  1.000 0.082 -     -     -     -    
# T8h  0.134 1.000 0.136 -     -     -    
# T3d  1.000 1.000 1.000 1.000 -     -    
# T7d  1.000 1.000 1.000 1.000 1.000 -    
# T14d 1.000 1.000 1.000 1.000 1.000 1.000

```

### Fig. 1 - Figure supplement 4C
```{r}
library(tidyverse) # v.2.0.0
library(PMCMRplus) # 1.9.4
path =  paste0(getwd(), "/Data/Physiological_measurements.tsv")
Weights = read_tsv(path)
Weights$Group <- factor(Weights$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

w = 0.25
OviductWeightb = ggplot(Weights, aes(y = `Oviduct (mg)`, x = Group)) + 
  geom_boxplot( outlier.colour = 'red', outlier.size = 0.5, width = 0.2) + 
  geom_point( position = position_nudge(x = w), size = 0.25) + 
  scale_color_manual(values = rep("black", 2)) +
  scale_y_continuous(name = "Oviduct weight (mg)" ) +
  theme_classic() ; OviductWeightb

# The Kruskal???Wallis test (One-way ANOVA on ranks)
kruskal.test(`Oviduct (mg)` ~ Group , data = Weights) 
# Kruskal-Wallis chi-squared = 11.58, df = 6, p-value = 0.07202
```

### Fig. 1 - Figure supplement 4D
```{r}
library(tidyverse) # v.2.0.0
library(PMCMRplus) # 1.9.4
path = paste0(getwd(), "/Data/HVC_volume.tsv")
HVC = read_tsv(path)
HVC$Group = factor(HVC$Group, levels = c("CON", "T1h", "T3h", "T8h", "T3d", "T7d", "T14d"))

w = 0.25
HVCnorb = ggplot(HVC, aes(y = HVC_nor, x = Group)) + 
  geom_boxplot( outlier.colour = 'red', outlier.size = 0.5, width = 0.2) + 
  geom_point( position = position_nudge(x = w), size = 0.25) + 
  scale_color_manual(values = rep("black", 2)) +
  scale_y_continuous(name = "Normalized HVC volume" ) +
  theme_classic() ; HVCnorb

# The Kruskal???Wallis test (One-way ANOVA on ranks)
kruskal.test(HVC_nor ~ Group , data = HVC) 
# Kruskal-Wallis chi-squared = 17.317, df = 6, p-value = 0.008186

kwAllPairsDunnTest(HVC_nor ~ Group, data = HVC, p.adjust.method = "holm") 
#        CON    T1h    T3h    T8h    T3d    T7d   
# T1h  1.0000 -      -      -      -      -     
# T3h  1.0000 1.0000 -      -      -      -     
# T8h  1.0000 1.0000 1.0000 -      -      -     
# T3d  1.0000 1.0000 1.0000 1.0000 -      -     
# T7d  1.0000 1.0000 1.0000 1.0000 1.0000 -     
# T14d 0.0442 0.0043 0.9625 0.0455 0.2068 0.4253
```
